{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d15f412-c42a-43cd-999d-81bba815d7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from src.architectures.feature_extractors.mobilenet import MobileNet\n",
    "from src.module.multiclass import MulticlassImageClassifier\n",
    "from src.architectures.head import MulticlassLinearClassificationHead\n",
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "count_params = lambda model: sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "seed_everything(42)\n",
    "img = torch.rand(1, 3, 224, 224)\n",
    "\n",
    "# summary(net, input_data=img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f15c870e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'loss_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss_fn'])`.\n",
      "  rank_zero_warn(\n",
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/pytorch_lightning/utilities/parsing.py:197: UserWarning: Attribute 'metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['metrics'])`.\n",
      "  rank_zero_warn(\n"
     ]
    }
   ],
   "source": [
    "v2_backbone = MobileNet(3, 1.0, \"v2\")\n",
    "v3_small_backbone = MobileNet(3, 1.0, \"v3_small\")\n",
    "v3_large_backbone = MobileNet(3, 1.0, \"v3_large\")\n",
    "\n",
    "\n",
    "v2 = MulticlassImageClassifier(\n",
    "    v2_backbone,\n",
    "    head = MulticlassLinearClassificationHead(v2_backbone.out_dim, num_classes=1000),\n",
    "    classes=[str(i) for i in range(1000)]\n",
    ").net\n",
    "\n",
    "v3_small = MulticlassImageClassifier(\n",
    "    v3_small_backbone,\n",
    "    head = MulticlassLinearClassificationHead(v3_small_backbone.out_dim, num_classes=1000),\n",
    "    classes=[str(i) for i in range(1000)]\n",
    ").net\n",
    "\n",
    "v3_large = MulticlassImageClassifier(\n",
    "    v3_large_backbone,\n",
    "    head = MulticlassLinearClassificationHead(v3_large_backbone.out_dim, num_classes=1000),\n",
    "    classes=[str(i) for i in range(1000)]\n",
    ").net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "711adbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import mobilenet_v2, mobilenet_v3_small, mobilenet_v3_large \n",
    "v2_torch = mobilenet_v2()\n",
    "v3_small_torch = mobilenet_v3_small()\n",
    "v3_large_torch = mobilenet_v3_large()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d04089e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5483032, 5483320)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_params(v3_large_torch), count_params(v3_large), "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9cac5138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "MobileNetV3                                        [1, 1000]                 --\n",
       "├─Sequential: 1-1                                  [1, 960, 7, 7]            --\n",
       "│    └─Conv2dNormActivation: 2-1                   [1, 16, 112, 112]         464\n",
       "│    └─InvertedResidual: 2-2                       [1, 16, 112, 112]         464\n",
       "│    └─InvertedResidual: 2-3                       [1, 24, 56, 56]           3,440\n",
       "│    └─InvertedResidual: 2-4                       [1, 24, 56, 56]           4,440\n",
       "│    └─InvertedResidual: 2-5                       [1, 40, 28, 28]           10,328\n",
       "│    └─InvertedResidual: 2-6                       [1, 40, 28, 28]           20,992\n",
       "│    └─InvertedResidual: 2-7                       [1, 40, 28, 28]           20,992\n",
       "│    └─InvertedResidual: 2-8                       [1, 80, 14, 14]           32,080\n",
       "│    └─InvertedResidual: 2-9                       [1, 80, 14, 14]           34,760\n",
       "│    └─InvertedResidual: 2-10                      [1, 80, 14, 14]           31,992\n",
       "│    └─InvertedResidual: 2-11                      [1, 80, 14, 14]           31,992\n",
       "│    └─InvertedResidual: 2-12                      [1, 112, 14, 14]          214,424\n",
       "│    └─InvertedResidual: 2-13                      [1, 112, 14, 14]          386,120\n",
       "│    └─InvertedResidual: 2-14                      [1, 160, 7, 7]            429,224\n",
       "│    └─InvertedResidual: 2-15                      [1, 160, 7, 7]            797,360\n",
       "│    └─InvertedResidual: 2-16                      [1, 160, 7, 7]            797,360\n",
       "│    └─Conv2dNormActivation: 2-17                  [1, 960, 7, 7]            155,520\n",
       "├─AdaptiveAvgPool2d: 1-2                           [1, 960, 1, 1]            --\n",
       "├─Sequential: 1-3                                  [1, 1000]                 --\n",
       "│    └─Linear: 2-18                                [1, 1280]                 1,230,080\n",
       "│    └─Hardswish: 2-19                             [1, 1280]                 --\n",
       "│    └─Dropout: 2-20                               [1, 1280]                 --\n",
       "│    └─Linear: 2-21                                [1, 1000]                 1,281,000\n",
       "====================================================================================================\n",
       "Total params: 5,483,032\n",
       "Trainable params: 5,483,032\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 216.62\n",
       "====================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 70.46\n",
       "Params size (MB): 21.93\n",
       "Estimated Total Size (MB): 92.99\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(v3_large_torch, input_data=img, depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c9d9d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/torchinfo/torchinfo.py:477: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  action_fn=lambda data: sys.getsizeof(data.storage()),\n",
      "/home/shate/.cache/pypoetry/virtualenvs/image-classification-5tGgTKFW-py3.11/lib/python3.11/site-packages/torch/storage.py:665: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return super().__sizeof__() + self.nbytes()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=========================================================================================================\n",
       "Layer (type:depth-idx)                                  Output Shape              Param #\n",
       "=========================================================================================================\n",
       "Sequential                                              [1, 1000]                 --\n",
       "├─MobileNetV3Large: 1-1                                 [1, 1280, 1, 1]           --\n",
       "│    └─Sequential: 2-1                                  --                        --\n",
       "│    │    └─CNNBlock: 3-1                               [1, 16, 112, 112]         464\n",
       "│    │    └─Bottleneck: 3-2                             [1, 16, 112, 112]         752\n",
       "│    │    └─Bottleneck: 3-3                             [1, 24, 56, 56]           3,440\n",
       "│    │    └─Bottleneck: 3-4                             [1, 24, 56, 56]           4,440\n",
       "│    │    └─Bottleneck: 3-5                             [1, 40, 28, 28]           10,328\n",
       "│    │    └─Bottleneck: 3-6                             [1, 40, 28, 28]           20,992\n",
       "│    │    └─Bottleneck: 3-7                             [1, 40, 28, 28]           20,992\n",
       "│    │    └─Bottleneck: 3-8                             [1, 80, 14, 14]           32,080\n",
       "│    │    └─Bottleneck: 3-9                             [1, 80, 14, 14]           34,760\n",
       "│    │    └─Bottleneck: 3-10                            [1, 80, 14, 14]           31,992\n",
       "│    │    └─Bottleneck: 3-11                            [1, 80, 14, 14]           31,992\n",
       "│    │    └─Bottleneck: 3-12                            [1, 112, 14, 14]          214,424\n",
       "│    │    └─Bottleneck: 3-13                            [1, 112, 14, 14]          386,120\n",
       "│    │    └─Bottleneck: 3-14                            [1, 160, 7, 7]            429,224\n",
       "│    │    └─Bottleneck: 3-15                            [1, 160, 7, 7]            797,360\n",
       "│    │    └─Bottleneck: 3-16                            [1, 160, 7, 7]            797,360\n",
       "│    │    └─EfficientLastStage: 3-17                    [1, 1280, 1, 1]           1,385,600\n",
       "├─MulticlassLinearClassificationHead: 1-2               [1, 1000]                 --\n",
       "│    └─Sequential: 2-2                                  [1, 1000]                 --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-18                     [1, 1280, 1, 1]           --\n",
       "│    │    └─Flatten: 3-19                               [1, 1280]                 --\n",
       "│    │    └─Dropout: 3-20                               [1, 1280]                 --\n",
       "│    │    └─Linear: 3-21                                [1, 1000]                 1,281,000\n",
       "│    └─LogSoftmax: 2-3                                  [1, 1000]                 --\n",
       "=========================================================================================================\n",
       "Total params: 5,483,320\n",
       "Trainable params: 5,483,320\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 219.83\n",
       "=========================================================================================================\n",
       "Input size (MB): 0.60\n",
       "Forward/backward pass size (MB): 73.67\n",
       "Params size (MB): 21.93\n",
       "Estimated Total Size (MB): 96.20\n",
       "========================================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(v3_large, input_data=img, depth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44027efc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dNormActivation(\n",
       "  (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "  (2): Hardswish()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_small_torch.features[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "972bcaaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientLastStage(\n",
       "  (conv_1): SEBlock(\n",
       "    (block): CNNBlock(\n",
       "      (conv): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (batch_norm): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (activation_fn): Hardswish()\n",
       "    )\n",
       "    (squeeze): Sequential(\n",
       "      (0): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (excitation): Sequential(\n",
       "      (0): Linear(in_features=576, out_features=144, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=144, out_features=576, bias=True)\n",
       "      (3): Hardsigmoid()\n",
       "    )\n",
       "  )\n",
       "  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (conv_2): CNNBlock(\n",
       "    (conv): Conv2d(576, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (activation_fn): Hardswish()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v3_small[0].net.last_stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d8a6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
